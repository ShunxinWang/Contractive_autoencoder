{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Regular Autoencoder with contractive loss (3 versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import grad, Variable,functional\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# ---------------------------------Model----------------------------------------------\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class Reshape(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(-1,512,8,8)\n",
    "\n",
    "class Regular_AE(nn.Module):\n",
    "    def __init__(self,laten_dims=64):\n",
    "        super(Regular_AE, self).__init__()\n",
    "        self.laten_dims = laten_dims\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128),           \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),     \n",
    "            nn.BatchNorm2d(256),     \n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(256, 512, 4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(512),         \n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(4*4*512,self.laten_dims)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(laten_dims,8*8*512),\n",
    "            Reshape(),\n",
    "\t\t\tnn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 3, 1, stride=1, padding=0),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Linear(self.laten_dims,64),\n",
    "            nn.Linear(64,32),\n",
    "            nn.Linear(32,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        prediction = self.classifer(encoded)\n",
    "        return encoded, decoded, prediction\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Contractive loss -- three versions \n",
    "### The first one calculates the loss element by element\n",
    "### The second one calculates the loss by $torch.autograd.grad(output,inputs,grad_output = torch.ones(output.size()))$ to make it calculate derivatives from a tensor to a tensor\n",
    "### The third one calculates the loss by $torch.autograd.functional.jacobian$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctr_lossv1(x, encoding):\n",
    "    contractive_loss = 0.0\n",
    "    for encoding_i in encoding:\n",
    "        grads = grad(encoding_i,x,create_graph=True)    \n",
    "        print(grads[0].shape)\n",
    "        list_grad.append(grads[0]) \n",
    "        contractive_loss  += sum([grd.norm()**2 for grd in grads[0]])\n",
    "    return contractive_loss\n",
    "\n",
    "def ctr_sum(x_batch,encoding_batch):\n",
    "    sum_ctr = 0.0\n",
    "    for i in range(len(x_batch)):\n",
    "        sum_ctr += ctr_lossv1(x_batch,encoding_batch[i])\n",
    "    return sum_ctr\n",
    "\n",
    "def ctr_lossv2(x, encoding):\n",
    "    contractive_loss = 0.0\n",
    "    grads = grad(encoding,x,torch.ones(encoding.size()),create_graph=True)\n",
    "    contractive_loss  += sum([grd.norm()**2 for grd in grads[0]])\n",
    "\n",
    "    return contractive_loss\n",
    "\n",
    "def ctr_lossv3(x,function):\n",
    "    matrix = functional.jacobian(function,x,create_graph=True)\n",
    "    return sum([grd.norm()**2 for grd in matrix])\n",
    "\n"
   ]
  },
  {
   "source": [
    "### $z = [z_1, z_2, ...z_{latent\\ dims}]$ --> $z = z_1+z_2+...+z_{latent\\ dims}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 32, 32])\n",
      "tensor(507.2557, grad_fn=<AddBackward0>)\n",
      "tensor(480.1156, grad_fn=<AddBackward0>)\n",
      "tensor(507.2443, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = Regular_AE()\n",
    "list_grad = []\n",
    "# batchsize = 4\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "#     ])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data÷sets',train=True, download=False,transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=True) #working in Windows, no need for num_workers = 2\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "wd = 5e-04\n",
    "optimizer = optim.Adam(net.parameters(),weight_decay=wd, lr=0.001)\n",
    "\n",
    "\n",
    "# for i, data in enumerate(trainloader,0):\n",
    "x = Variable(torch.rand((2,3,32,32)), requires_grad=True)            \n",
    "optimizer.zero_grad()\n",
    "encoding, decoding, prediction = net(x)\n",
    "\n",
    "\n",
    "fc = ctr_sum(x,encoding)\n",
    "fc2 = ctr_lossv2(x, encoding)\n",
    "fc3 = ctr_lossv3(x, net.encoder)\n",
    "fc3.backward()\n",
    "\n",
    "recons_loss = criterion(decoding,x.detach())\n",
    "# loss = criterion2(prediction,torch.ones(2)) \n",
    "loss = recons_loss\n",
    "# print(x.grad)\n",
    "loss.backward()\n",
    "# print(x.grad)\n",
    "x.grad = None\n",
    "# Before the optimizer, x.grad needs to be None\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(fc)\n",
    "print(fc2)\n",
    "print(fc3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-2.6569e-04, -2.5591e-04, -9.0400e-03,  ...,  1.3941e-03,\n            1.6072e-03, -4.3095e-03],\n          [-7.9071e-03, -1.4946e-03, -1.1981e-02,  ..., -1.5065e-03,\n           -1.0530e-02,  1.7524e-02],\n          [-2.4777e-03,  1.8861e-03, -8.7394e-03,  ...,  1.9678e-02,\n            7.5352e-03, -1.8885e-02],\n          ...,\n          [ 1.6650e-02, -1.7192e-02,  2.0289e-02,  ...,  1.7918e-02,\n           -5.4475e-03, -7.1331e-03],\n          [-2.6177e-03,  2.8667e-02, -3.4633e-02,  ..., -2.5768e-02,\n           -4.5416e-03, -3.6562e-03],\n          [-3.4567e-03,  4.4974e-03, -2.1725e-03,  ..., -6.8672e-03,\n            6.8729e-03, -4.0374e-03]],\n\n         [[ 3.0583e-03,  9.0307e-03,  1.3011e-02,  ..., -9.4028e-03,\n           -2.4438e-03, -1.7759e-03],\n          [-2.6144e-03,  2.7208e-02,  4.3871e-03,  ..., -1.2303e-03,\n           -1.4556e-02,  1.6034e-03],\n          [-1.0238e-02,  4.1496e-03, -4.6915e-02,  ...,  1.3503e-02,\n            1.8471e-02, -9.5419e-03],\n          ...,\n          [ 1.6777e-02, -2.7725e-02,  4.9582e-03,  ..., -1.6183e-02,\n           -9.7142e-03, -4.0625e-03],\n          [ 1.5542e-02, -5.8852e-03, -9.5978e-03,  ...,  8.5004e-03,\n            2.1103e-02,  3.3545e-03],\n          [ 2.5215e-03,  1.0789e-02,  7.2441e-03,  ..., -6.1751e-03,\n            1.0460e-03,  1.8944e-03]],\n\n         [[ 6.2907e-03, -8.2960e-03,  3.4711e-05,  ...,  2.1305e-03,\n            1.4262e-02, -4.8295e-03],\n          [-1.4340e-02, -1.1443e-02, -8.2812e-03,  ...,  3.9128e-02,\n           -1.2312e-02, -1.3984e-02],\n          [-9.8220e-03,  8.3533e-03, -2.2189e-03,  ..., -3.0728e-04,\n           -1.2038e-02,  8.7810e-03],\n          ...,\n          [-1.1952e-03,  4.3644e-03,  7.7017e-03,  ..., -1.6423e-02,\n           -2.1275e-03, -7.2874e-03],\n          [ 9.2349e-03,  2.2763e-02, -4.5749e-02,  ..., -2.8513e-02,\n           -1.7166e-02, -1.8785e-02],\n          [-2.5667e-03,  6.3018e-03, -1.4358e-02,  ...,  7.9939e-03,\n           -1.2498e-02, -5.9708e-03]]],\n\n\n        [[[-1.2981e-04, -4.2947e-03,  1.4806e-03,  ...,  1.3809e-03,\n            1.3803e-03,  6.8248e-04],\n          [-4.0460e-04, -4.8609e-03, -5.3575e-03,  ...,  2.5282e-03,\n            1.9527e-03,  3.2037e-03],\n          [-7.6455e-05, -2.6570e-03, -7.6260e-03,  ..., -2.7525e-03,\n            4.6901e-03,  1.6916e-03],\n          ...,\n          [ 8.7052e-04,  4.7009e-03,  3.2743e-03,  ...,  2.1529e-04,\n            3.1353e-03,  1.2466e-05],\n          [-6.6180e-04,  2.5541e-03, -2.8583e-03,  ...,  3.1461e-03,\n            1.1300e-03,  4.1643e-04],\n          [ 8.7407e-04, -2.5183e-03, -2.9884e-04,  ...,  1.3423e-03,\n            1.5802e-03, -5.3589e-04]],\n\n         [[ 1.4379e-03,  2.3146e-03,  3.7482e-03,  ..., -2.4222e-03,\n            5.6632e-04,  5.8845e-04],\n          [ 3.3297e-03,  4.2293e-04,  1.3659e-02,  ...,  4.1895e-03,\n           -4.1541e-03,  1.6765e-03],\n          [-6.9357e-04, -4.8140e-03,  6.2752e-03,  ...,  2.4115e-03,\n           -6.9052e-03,  9.0577e-04],\n          ...,\n          [-3.3979e-03, -7.0102e-03, -6.3398e-03,  ...,  1.2367e-02,\n            5.1617e-03, -1.6377e-03],\n          [-1.6074e-04, -2.0149e-03, -1.4810e-04,  ..., -3.2406e-03,\n           -1.3350e-03,  5.1439e-04],\n          [ 1.3432e-04,  5.8366e-04, -3.3183e-03,  ..., -1.5979e-03,\n            6.9911e-04,  1.2479e-03]],\n\n         [[-2.2760e-03,  1.7181e-03, -1.2993e-03,  ...,  1.3112e-03,\n            7.0931e-04, -1.8608e-03],\n          [ 4.1488e-04,  2.6370e-03, -1.1016e-03,  ..., -1.8701e-03,\n            2.3710e-04, -1.1392e-03],\n          [ 2.3927e-03,  1.4615e-03,  8.2368e-04,  ..., -5.1406e-03,\n            4.3534e-03, -1.3773e-03],\n          ...,\n          [ 4.2032e-04, -6.2310e-04,  1.9531e-03,  ..., -1.1358e-03,\n            3.2214e-03, -9.1456e-04],\n          [ 3.8817e-03,  1.8134e-03,  2.4336e-03,  ...,  2.4601e-03,\n           -1.2413e-03, -5.4449e-03],\n          [-2.6036e-03, -4.0241e-04, -1.6967e-04,  ...,  1.2486e-03,\n           -8.0208e-04, -1.5236e-03]]]],\n       grad_fn=<MkldnnConvolutionBackwardBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(list_grad[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.The batch size cannot be large, otherwise the GPU is out of memory (Heavy computation)\n",
    "### 2. Version 2 is not correct; it calcultes $\\partial (z_1+z_2+...+z_{dim}) /\\partial x_i$, where $z$ is a scalar not a vector. The size of the calculated gradients is the same as the input's size\n",
    "### 3. The difference between the losses calculated by version 1 and version 3 is small when the batch size is small.\n",
    "#### What to notice is: the gradients of inputs need to be cleaned before $optimizer.step()$, otherwise the inputs will be updated, just like adding adversarial perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 16\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data÷sets',train=True, download=False,transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchsize, shuffle=True) #working in Windows, no need for num_workers = 2\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "wd = 5e-04\n",
    "optimizer = optim.Adam(net.parameters(),weight_decay=wd, lr=0.001)\n",
    "\n",
    "\n",
    "# for i, data in enumerate(trainloader,0):\n",
    "x = Variable(torch.rand((batchsize,3,32,32)), requires_grad=True)            \n",
    "optimizer.zero_grad()\n",
    "encoding, decoding, prediction = net(x)\n",
    "\n",
    "\n",
    "fc = ctr_sum(x,encoding)\n",
    "fc2 = ctr_lossv2(x, encoding)\n",
    "fc3 = ctr_lossv3(x, net.encoder)\n",
    "fc3.backward()\n",
    "\n",
    "recons_loss = criterion(decoding,x.detach())\n",
    "# loss = criterion2(prediction,torch.ones(2)) \n",
    "loss = recons_loss\n",
    "# print(x.grad)\n",
    "loss.backward()\n",
    "# print(x.grad)\n",
    "x.grad = None\n",
    "# Before the optimizer, x.grad needs to be None\n",
    "optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(149.8658, grad_fn=<AddBackward0>)\n",
      "tensor(131.9954, grad_fn=<AddBackward0>)\n",
      "tensor(149.5394, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(fc)\n",
    "print(fc2)\n",
    "print(fc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Observation: \n",
    "### The contractive loss is calulated from $\\frac{\\partial encoding_{x_i}}{\\partial x_i}$ \n",
    "### And $\\frac{\\partial encoding_{x_i}}{\\partial x_j} = 0$, where $i \\neq j$\n",
    "### But it is observed that the derivative of the encoding of image $x_i$ with respect to other images in the same batch $x_j$  is not zero. \n",
    "### Therefore, it raises a problem: whether to force images in the same batch belonging to the same category or just similar to contrastive loss, to make images with different augmentations in the same batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}