{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Regular Autoencoder with contractive loss (Implemented by 2 ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import grad, Variable,functional\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# ---------------------------------Model----------------------------------------------\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class Reshape(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(-1,512,8,8)\n",
    "\n",
    "class Regular_AE(nn.Module):\n",
    "    def __init__(self,laten_dims=64):\n",
    "        super(Regular_AE, self).__init__()\n",
    "        self.laten_dims = laten_dims\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 4, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(128),           \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),     \n",
    "            nn.BatchNorm2d(256),     \n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(256, 512, 4, stride=2, padding=1),  \n",
    "            nn.BatchNorm2d(512),         \n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(4*4*512,self.laten_dims)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(laten_dims,8*8*512),\n",
    "            Reshape(),\n",
    "\t\t\tnn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 3, 1, stride=1, padding=0),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded,decoded\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated element by element\n",
    "def ctr_lossv1(x, encoding):\n",
    "    contractive_loss = 0.0\n",
    "    for encoding_i in encoding:\n",
    "        grads = grad(encoding_i,x,create_graph=True)    \n",
    "        list_grad.append(grads[0]) \n",
    "        contractive_loss  += sum([grd.norm()**2 for grd in grads[0]])\n",
    "    return contractive_loss\n",
    "\n",
    "def ctr_sum(x_batch,encoding_batch):\n",
    "    sum_ctr = 0.0\n",
    "    for i in range(len(x_batch)):\n",
    "        sum_ctr += ctr_lossv1(x_batch,encoding_batch[i])\n",
    "    return sum_ctr\n",
    "# calculated by pytorch jacobian function\n",
    "\n",
    "def ctr_lossv3(x,function):\n",
    "    matrix = functional.jacobian(function,x,create_graph=True)\n",
    "    return sum([grd.norm()**2 for grd in matrix])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(516.3995, grad_fn=<AddBackward0>)\ntensor(516.3872, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = Regular_AE()\n",
    "list_grad = []\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "num_epochs = 100\n",
    "wd = 5e-04\n",
    "optimizer = optim.Adam(net.parameters(),weight_decay=wd, lr=0.001)\n",
    "\n",
    "\n",
    "# for i, data in enumerate(trainloader,0):\n",
    "x = Variable(torch.rand((2,3,32,32)), requires_grad=True)            \n",
    "optimizer.zero_grad()\n",
    "encoding, decoding = net(x)\n",
    "\n",
    "\n",
    "fc = ctr_sum(x,encoding)\n",
    "fc3 = ctr_lossv3(x, net.encoder)\n",
    "fc3.backward()\n",
    "\n",
    "\n",
    "loss = criterion(decoding,x.detach())\n",
    "loss.backward()\n",
    "x.grad = None\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(fc)\n",
    "print(fc3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}